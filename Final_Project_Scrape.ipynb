{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: This is largely pulled from QBias' datacrawl, AllsidesDataCrawl.ipynb, which they used to scrape links from a table and, subsequently, the associated content from AllSides' news' roundup through to winter of 2022 (https://github.com/irgroup/Qbias/blob/main/Scraper/AllsidesDataCrawl.ipynb)\n",
        "\n",
        "Unfortunately, AllSides seems to have updated its site and the scraper (with Selenium) as written triggers a TimeoutException when attempting to scrape links from AllSides' complied table. Attempts to conditionally handle cookie popups and similar, change waittimes, update the pagination button class, and use a more generic Xpath for the table doesn't resolve this, as the scraper fails to gauge the correct number of pages.\n",
        "\n",
        "An attempt to use BeautifulSoup didn't seem to work as well. Based on what I can gauge on the topic via StackOverflow, the implication seems to be that the table isn't in the static HTML, as JavaScript fetches & renders it after the page loads, blocking the content.\n",
        "\n",
        "Consequently, *this file will not run successfully*; I've included it here to demonstrate the source of the 2022 data and document my attempts at creating a new scraper.\n"
      ],
      "metadata": {
        "id": "LtYz7x1F_vGx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SW5UHwJPLwC2"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget -O /tmp/chrome.deb https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "#!dpkg -i /tmp/chrome.deb\n",
        "#!apt-get -f install -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8Kvwv8JOHx_",
        "outputId": "7ee18b85-eeea-49db-9b39-33327cfeb30f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-08 05:29:00--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 142.251.189.190, 142.251.189.93, 142.251.189.91, ...\n",
            "Connecting to dl.google.com (dl.google.com)|142.251.189.190|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 114757600 (109M) [application/x-debian-package]\n",
            "Saving to: ‘/tmp/chrome.deb’\n",
            "\n",
            "/tmp/chrome.deb     100%[===================>] 109.44M   352MB/s    in 0.3s    \n",
            "\n",
            "2025-03-08 05:29:00 (352 MB/s) - ‘/tmp/chrome.deb’ saved [114757600/114757600]\n",
            "\n",
            "Selecting previously unselected package google-chrome-stable.\n",
            "(Reading database ... 125113 files and directories currently installed.)\n",
            "Preparing to unpack /tmp/chrome.deb ...\n",
            "Unpacking google-chrome-stable (134.0.6998.35-1) ...\n",
            "\u001b[1mdpkg:\u001b[0m dependency problems prevent configuration of google-chrome-stable:\n",
            " google-chrome-stable depends on libvulkan1; however:\n",
            "  Package libvulkan1 is not installed.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m error processing package google-chrome-stable (--install):\n",
            " dependency problems - leaving unconfigured\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Errors were encountered while processing:\n",
            " google-chrome-stable\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Correcting dependencies... Done\n",
            "The following additional packages will be installed:\n",
            "  libvulkan1 mesa-vulkan-drivers\n",
            "The following NEW packages will be installed:\n",
            "  libvulkan1 mesa-vulkan-drivers\n",
            "0 upgraded, 2 newly installed, 0 to remove and 31 not upgraded.\n",
            "1 not fully installed or removed.\n",
            "Need to get 10.9 MB of archives.\n",
            "After this operation, 51.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvulkan1 amd64 1.3.204.1-2 [128 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 mesa-vulkan-drivers amd64 23.2.1-1ubuntu3.1~22.04.3 [10.7 MB]\n",
            "Fetched 10.9 MB in 2s (6,964 kB/s)\n",
            "Selecting previously unselected package libvulkan1:amd64.\n",
            "(Reading database ... 125230 files and directories currently installed.)\n",
            "Preparing to unpack .../libvulkan1_1.3.204.1-2_amd64.deb ...\n",
            "Unpacking libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Selecting previously unselected package mesa-vulkan-drivers:amd64.\n",
            "Preparing to unpack .../mesa-vulkan-drivers_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Setting up mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up google-chrome-stable (134.0.6998.35-1) ...\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/google-chrome (google-chrome) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt-get update\n",
        "# !apt-get install -y wget unzip\n",
        "# !apt-get install -y google-chrome-stable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0Ylkfg2Piko",
        "outputId": "c3bf4ee7-8531-4e70-9696-288ea7a6aad6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://dl.google.com/linux/chrome/deb stable InRelease [1,825 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connected to cloud.r-project.org (18.16\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connected to cloud.r-project.org (18.16\r                                                                                                    \rHit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 https://dl.google.com/linux/chrome/deb stable/main amd64 Packages [1,211 B]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 3,036 B in 2s (1,824 B/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "google-chrome-stable is already the newest version (134.0.6998.35-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !google-chrome --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhkfaUKyPnwt",
        "outputId": "35146eb6-3799-468c-8223-48a051cd11dc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Chrome 134.0.6998.35 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with the correct ChromeDriver link for your version!\n",
        "chromedriver_url = \"https://storage.googleapis.com/chrome-for-testing-public/134.0.6998.35/linux64/chromedriver-linux64.zip\"\n",
        "\n",
        "# Download and install ChromeDriver\n",
        "!wget -q -O /tmp/chromedriver.zip {chromedriver_url}\n",
        "!unzip -o /tmp/chromedriver.zip -d /tmp/\n",
        "!chmod +x /tmp/chromedriver-linux64/chromedriver\n",
        "!mv /tmp/chromedriver-linux64/chromedriver /usr/bin/chromedriver\n",
        "\n",
        "# Verify the installation\n",
        "!chromedriver --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa1JbmelQPGa",
        "outputId": "a5e3ea0a-885a-4285-e615-d5b10435a0b6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /tmp/chromedriver.zip\n",
            "  inflating: /tmp/chromedriver-linux64/LICENSE.chromedriver  \n",
            "  inflating: /tmp/chromedriver-linux64/THIRD_PARTY_NOTICES.chromedriver  \n",
            "  inflating: /tmp/chromedriver-linux64/chromedriver  \n",
            "ChromeDriver 134.0.6998.35 (ea6ef4c2ac15ae95d2cfd65682da62c093415099-refs/branch-heads/6998@{#1472})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service"
      ],
      "metadata": {
        "id": "Tkn0H2BrL1RU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options"
      ],
      "metadata": {
        "id": "SQhNAvkEO7Na"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set paths for Chrome and ChromeDriver\n",
        "chrome_path = \"/usr/bin/google-chrome\"\n",
        "chromedriver_path = \"/usr/bin/chromedriver\"\n",
        "\n",
        "# Configure Chrome options\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Run without GUI\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.binary_location = chrome_path  # Set correct Chrome binary\n",
        "\n",
        "# Set up WebDriver\n",
        "service = Service(chromedriver_path)\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Test if it works\n",
        "driver.get(\"https://www.google.com\")\n",
        "print(\"Page title:\", driver.title)\n",
        "\n",
        "# Close the driver\n",
        "#driver.quit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmcJB2ZUOQl4",
        "outputId": "e6c7e402-92ee-4340-9f49-3eb2ae1285bb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page title: Google\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wait_time = 0.50"
      ],
      "metadata": {
        "id": "LxSRYhBwMLSF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chromedriver setup\n",
        "\n",
        "serv = Service(r'driver/chromedriver') #path from 'which chromedriver'\n",
        "\n",
        "chrome_options = Options()\n",
        "\n",
        "chrome_options.add_argument(\"--headless\")"
      ],
      "metadata": {
        "id": "NQYdFrxVMU1G"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assemble list of links to all articles\n",
        "links = []\n",
        "\n",
        "# interact with cookie terms\n",
        "chrome_path = r'driver/chromedriver' #path from 'which chromedriver'\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\") # open chrome in background\n",
        "driver = webdriver.Chrome(service=serv, options=chrome_options)\n",
        "driver.get('https://www.allsides.com/headline-roundups')\n",
        "wait = WebDriverWait(driver, 10)\n",
        "wait.until(EC.element_to_be_clickable((By.CLASS_NAME, 'css-47sehv')))\n",
        "\n",
        "ele = driver.find_element(By.CLASS_NAME, \"css-47sehv\")\n",
        "ele.click()\n",
        "\n",
        "# retireve number of pages\n",
        "last_page_button = driver.find_element(By.CLASS_NAME, \"pager-last\")\n",
        "link_last_page = last_page_button.find_elements(By.TAG_NAME, \"a\")\n",
        "t = link_last_page[0].get_attribute(\"href\")\n",
        "last_page_index = int(t[-3:])\n",
        "\n",
        "# retrieve links from start page\n",
        "main_table = driver.find_element(By.XPATH, \"//*[@id=\\\"block-views-de37fa32ea86f5545eb9b7722977a70d\\\"]/div/div[2]/table/tbody\") # table body\n",
        "\n",
        "rows = main_table.find_elements(By.TAG_NAME, \"tr\")\n",
        "for i in rows:\n",
        "    entry = i.find_elements(By.TAG_NAME, \"td\")\n",
        "    link = entry[0].find_element(By.TAG_NAME, \"a\")\n",
        "    links.append(link.get_attribute(\"href\"))\n",
        "WebDriverWait(driver, 20)\n",
        "\n",
        "# retrieve links for other pages\n",
        "for page in tqdm(range(2,last_page_index+1)):   # set to max number of pages\n",
        "    driver.get(\"https://www.allsides.com/headline-roundups?page=\"+str(page))\n",
        "    WebDriverWait(driver, 20)\n",
        "    main_table = driver.find_element(By.XPATH, \"//*[@id=\\\"block-views-de37fa32ea86f5545eb9b7722977a70d\\\"]/div/div[2]/table/tbody\") # table body\n",
        "\n",
        "    rows = main_table.find_elements(By.TAG_NAME, \"tr\")\n",
        "    for i in rows:\n",
        "        entry = i.find_elements(By.TAG_NAME, \"td\")\n",
        "        link = entry[0].find_element(By.TAG_NAME, \"a\")\n",
        "        links.append(link.get_attribute(\"href\"))\n",
        "\n",
        "driver.close()"
      ],
      "metadata": {
        "id": "ZV26x6Fv_nqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Troubleshooting Link Retrieval: - CF\n",
        "\n",
        "import os\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "\n",
        "#Initialize WebDriver\n",
        "chrome_path = \"/usr/bin/chromedriver\"\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "service = Service(chrome_path)\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "#Open Website\n",
        "driver.get(\"https://www.allsides.com/headline-roundups\")\n",
        "wait = WebDriverWait(driver, 20)  #Increased timeout\n",
        "\n",
        "#Force JavaScript to Render Page\n",
        "def wait_for_js_load():\n",
        "    time.sleep(3)\n",
        "    for _ in range(10):  #Scroll multiple times to load content\n",
        "        driver.execute_script(\"window.scrollBy(0, document.body.scrollHeight);\")\n",
        "        time.sleep(2)  #Allow content to load\n",
        "\n",
        "    #Ensure the page is truly loaded before proceeding\n",
        "    wait.until(lambda d: d.execute_script(\"return document.readyState\") == \"complete\")\n",
        "    time.sleep(3)  #Extra wait\n",
        "\n",
        "wait_for_js_load()\n",
        "\n",
        "#Save Screenshots with File Existence Check\n",
        "screenshot_path_before = \"/content/debug_before.png\"\n",
        "screenshot_path_after = \"/content/debug_after.png\"\n",
        "\n",
        "driver.save_screenshot(screenshot_path_before)\n",
        "if os.path.exists(screenshot_path_before):\n",
        "    print(f\"Screenshot successfully saved: {screenshot_path_before}\")\n",
        "else:\n",
        "    print(\"Screenshot saving failed!\")\n",
        "\n",
        "#Wait for Table to Appear Before Screenshot\n",
        "try:\n",
        "    print(\"Waiting for table...\")\n",
        "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"table.views-table.cols-3\")))\n",
        "    print(\"Table found! Taking another screenshot...\")\n",
        "\n",
        "    driver.save_screenshot(screenshot_path_after)\n",
        "    if os.path.exists(screenshot_path_after):\n",
        "        print(f\"Screenshot successfully saved: {screenshot_path_after}\")\n",
        "    else:\n",
        "        print(\"Screenshot saving failed!\")\n",
        "except:\n",
        "    print(\"Table STILL not found after waiting!\")\n",
        "\n",
        "#Close WebDriver\n",
        "driver.quit()\n",
        "\n",
        "#List Files Again\n",
        "print(\"\\n Checking saved files in /content/:\")\n",
        "os.system(\"ls -lh /content/\")\n"
      ],
      "metadata": {
        "id": "7Oyz6Wx-MJbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Alternative: BeautifulSoup -CF\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "base_url = \"https://www.allsides.com/headline-roundups\"\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36\"\n",
        "}\n",
        "\n",
        "response = requests.get(base_url, headers=headers)\n",
        "\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "table = soup.find(\"table\", class_=\"views-table cols-3\")\n",
        "\n",
        "if table:\n",
        "    print(\"Table found, Extracting links...\")\n",
        "    links = []\n",
        "\n",
        "    for row in table.find_all(\"tr\"):\n",
        "        link_tag = row.find(\"td\", class_=\"views-field views-field-name\")\n",
        "        if link_tag:\n",
        "            a_tag = link_tag.find(\"a\")\n",
        "            if a_tag and \"href\" in a_tag.attrs:\n",
        "                links.append(\"https://www.allsides.com\" + a_tag[\"href\"])\n",
        "\n",
        "    print(f\"Scraped {len(links)} article links:\")\n",
        "    print(links[:5])\n",
        "else:\n",
        "    print(\"Table not found! JavaScript might be blocking the content.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "brMVcxgkL3Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# exports links as pickle file\n",
        "with open(\"linklist_allsides_news.pickle\", \"wb\") as f:\n",
        "    pickle.dump(links, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "1rVP3kQ_NAwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# exports links as csv\n",
        "with open(\"linklist_allsides_news.csv\", \"w\") as f:\n",
        "    for line in links:\n",
        "        print(line, file=f)"
      ],
      "metadata": {
        "id": "AboRPDVCNBMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# functions that assert the existence of\n",
        "def check_exists_by_xpath(xpath):\n",
        "    try:\n",
        "        driver.find_elements(By.XPATH, xpath)[0]\n",
        "    except:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def check_exists_by_class(inp):\n",
        "    try:\n",
        "        driver.find_elements(By.CLASS_NAME, inp)[0]\n",
        "    except:\n",
        "        return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "CwTlvgaCND44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load link list from pickle file\n",
        "with open(\"linklist_allsides_news.pickle\", \"rb\") as f:\n",
        "    links = pickle.load(f)"
      ],
      "metadata": {
        "id": "VMVUImhKNIOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list for results\n",
        "data = []\n",
        "\n",
        "# retrieve information from articles in list of links\n",
        "for li in tqdm(links):\n",
        "    time.sleep(wait_time)\n",
        "    driver = webdriver.Chrome(service=serv, options=chrome_options)\n",
        "    print(li)\n",
        "    # open URL\n",
        "    driver.get(li)\n",
        "\n",
        "    # interact with pop-up window\n",
        "    if check_exists_by_class(\"css-47sehv\"):\n",
        "        ele = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((driver.find_element(By.CLASS_NAME, \"css-47sehv\"))))\n",
        "        ele.click()\n",
        "    else:\n",
        "        print(\"no button\")\n",
        "\n",
        "    # netral title heading\n",
        "    try:\n",
        "        heading = driver.find_element(By.TAG_NAME, \"h1\").text\n",
        "    except:\n",
        "        print(\"no heading found\")\n",
        "\n",
        "    print(heading)\n",
        "\n",
        "    # date\n",
        "    try:\n",
        "        date = driver.find_element(By.CLASS_NAME, \"date-display-single\").text\n",
        "    except:\n",
        "        date = \"\"\n",
        "\n",
        "    # tags\n",
        "    try:\n",
        "        tags = [a.text for a in driver.find_element(By.CLASS_NAME, \"page-tags\").find_elements(By.TAG_NAME, \"a\")]\n",
        "    except:\n",
        "        tags = \"\"\n",
        "\n",
        "\n",
        "    # define XPATH inforamtion for article divs\n",
        "    divs = [\"/html/body/div[4]/div/div/div/div[4]/div/div/div/div[1]\", \"/html/body/div[4]/div/div/div/div[4]/div/div/div/div[2]\", \"/html/body/div[4]/div/div/div/div[4]/div/div/div/div[3]\"]\n",
        "\n",
        "    # access information in article divs\n",
        "    for d in divs:\n",
        "        if check_exists_by_xpath(d):\n",
        "            div = driver.find_elements(By.XPATH, d)[0]\n",
        "\n",
        "            # check heading element to find out left/center/right. The title contains the bias label that we can retrieve from the text here\n",
        "            try:\n",
        "                cat = div.find_element(By.TAG_NAME, \"h3\").text\n",
        "            except:\n",
        "                print(\"no headline found\")\n",
        "\n",
        "            # retrieve link to original article\n",
        "            try:\n",
        "                link = div.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
        "                print(link)\n",
        "            except:\n",
        "                print(\"no link found\")\n",
        "\n",
        "            # left/center/right are shuffled for each article, some roundups have e.g. only left and right articles.\n",
        "            # Thus, we have look to look each article seperately\n",
        "            time.sleep(0.2)\n",
        "            if \"Left\" in cat:\n",
        "                print(\"left\")\n",
        "                try:\n",
        "                    left_heading = div.find_element(By.CLASS_NAME, \"news-title\").text # heading\n",
        "                except:\n",
        "                    left_heading = \"\"\n",
        "                    print(\"no headline found\")\n",
        "                try:\n",
        "                    left_source = div.find_element(By.CLASS_NAME, \"source-area\").find_element(By.TAG_NAME, \"span\").text #source\n",
        "                except:\n",
        "                    left_source = \"\"\n",
        "                    print(\"no source found\")\n",
        "                try:\n",
        "                    left_text = div.find_element(By.CLASS_NAME, \"news-body\").find_element(By.CLASS_NAME, \"body-contents\").text # news text body-contents\n",
        "                except:\n",
        "                    left_text = \"\"\n",
        "                    print(\"no text found\")\n",
        "\n",
        "                # add the article information\n",
        "                data.append({\"url\":link, \"date\":date, \"title\": heading, \"tags\": tags, \"heading\":left_heading, \"source\": left_source, \"text\": left_text, \"bias_rating\": \"left\"})\n",
        "\n",
        "            elif \"Right\" in cat:\n",
        "                print(\"right\")\n",
        "                try:\n",
        "                    right_heading = div.find_element(By.CLASS_NAME, \"news-title\").text # heading\n",
        "                except:\n",
        "                    right_heading = \"\"\n",
        "                    print(\"no headline found\")\n",
        "                try:\n",
        "                    right_source = div.find_element(By.CLASS_NAME, \"source-area\").find_element(By.TAG_NAME, \"span\").text #source\n",
        "                except:\n",
        "                    right_source = \"\"\n",
        "                    print(\"no source found\")\n",
        "                try:\n",
        "                    right_text = div.find_element(By.CLASS_NAME, \"news-body\").find_element(By.CLASS_NAME, \"body-contents\").text # news text\n",
        "                except:\n",
        "                    right_text = \"\"\n",
        "                    print(\"no text found\")\n",
        "\n",
        "                # add the article information\n",
        "                data.append({\"url\":link, \"date\":date, \"title\": heading, \"tags\": tags, \"heading\":right_heading, \"source\": right_source, \"text\": right_text, \"bias_rating\": \"right\"})\n",
        "\n",
        "            else:\n",
        "                print(\"center\")\n",
        "                try:\n",
        "                    center_heading = div.find_element(By.CLASS_NAME, \"news-title\").text # heading\n",
        "                except:\n",
        "                    center_heading = \"\"\n",
        "                    print(\"no headline found\")\n",
        "                try:\n",
        "                    center_source = div.find_element(By.CLASS_NAME, \"source-area\").find_element(By.TAG_NAME, \"span\").text #source\n",
        "                except:\n",
        "                    center_source = \"\"\n",
        "                    print(\"no source found\")\n",
        "                try:\n",
        "                    center_text = div.find_element(By.CLASS_NAME, \"news-body\").find_element(By.CLASS_NAME, \"body-contents\").text # news text\n",
        "                except:\n",
        "                    center_text = \"\"\n",
        "                    print(\"no text found\")\n",
        "\n",
        "                # add the article information\n",
        "                data.append({\"url\":link, \"date\":date, \"title\": heading, \"tags\": tags, \"heading\":center_heading, \"source\": center_source, \"text\": center_text, \"bias_rating\": \"center\"})\n",
        "        else:\n",
        "            print(\"div not found\")\n",
        "\n",
        "    driver.close()\n",
        "\n",
        "    # clear output\n",
        "    clear_output()\n",
        "    # added a wait here to assert the scraper runs well\n",
        "    time.sleep(wait_time)"
      ],
      "metadata": {
        "id": "csA5YQDNNME3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## convert data to dataframe\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# export scraped articles\n",
        "df.to_csv(\"allsides_news_complete.csv\")"
      ],
      "metadata": {
        "id": "M-P1IACbNMlJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}